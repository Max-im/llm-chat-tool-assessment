# LLM Chat Tool Assessment

This is a minimal full-stack chat application built with **Next.js App Router**, **TailwindCSS**. It supports streaming LLM responses.

## âœ¨ Features

- Clean, responsive chat UI (`/chat` route)
- Real-time streamed responses from an LLM
- Error handling for invalid input
- Local chat history

---

## ðŸš€ Live Demo

[https://your-vercel-deployment-url.vercel.app/chat](https://your-vercel-deployment-url.vercel.app/chat)

---

## ðŸ“¦ Tech Stack

- Next.js (App Router)
- TailwindCSS
- TypeScript

---

## ðŸ”§ Setup Instructions

1. **Clone the repository**

```bash
git clone https://github.com/max-im/llm-chat-tool-assessment.git
cd llm-chat-tool-assessment
```

2. **Make up `.env.local` file from `env.local.sample`, put your own openai api key**

3. **Run `npm run dev`**
